---
title: "LLM Benchmark"
date: 2025-06-26T18:49:48+08:00
# bookComments: false
# bookSearchExclude: false
---

# LLM评测榜单集锦

## 文本生成能力

| 榜单名称      | 简介                                                         | 侧重能力                                     |
| ------------- | ------------------------------------------------------------ | -------------------------------------------- |
| AIME          | 美国数学邀请赛（American Invitational Mathematics Examination），高难度的数学竞赛，主要是给那些在 AMC（美国数学竞赛）中表现出色的高中生准备的。 | 高级数学推理、多步骤计算、竞赛级数学问题解决 |
| BeyondAIME    | 字节发布的高级数学推理基准                                   | 高级数学推理                                 |
| GPQA-Diamond  | 研究生级别问题回答基准（Graduate-Level Problem-Solving Questions，GPQA），评估LLM在生物、物理、化学等科学领域的专业知识和推理能力。GPQA Diamond是GPQA系列中最高质量的评测数据，GPQA系列有个最大的特点是精心设计，无法使用Google解决。 | 科学推理、专业知识理解、跨学科综合能力       |
| LiveCodeBench | 动态更新的基准测试平台，数据来自顶级竞赛平台的高难度编程任务 | 代码生成                                     |
| SWE-bench     | 从12个受欢迎的Python仓库中选出的、源自真实GitHub问题和相应拉取请求的2,294个软件工程问题。 | 真实世界的软件工程问题解决                   |
| TAU-Bench     | 是一个评估AI智能体在真实世界环境中可靠性的基准测试。它评估智能体是否能够在动态的多轮对话中与用户进行交互，理解需求并完成任务。 | 真实环境中AI智能体的可靠性                   |



## 视觉能力

| 榜单名称    | 简介                                                         | 侧重能力         |
| ----------- | ------------------------------------------------------------ | ---------------- |
| MMMU        | 收集了来自大学考试、测验和教科书的11.5K个多模态问题，跨越艺术与设计、商业、科学、健康医学、人文社会科学、技术工程等30个学科和183个子领域 | 多模态理解和推理 |
| MMMU-pro    | MMMU的升级版，引入了更严格的评估方法                         | 多模态理解和推理 |
| MathVision  | 数学题数据集                                                 | 数学能力         |
| RealWorldQA | 真实世界场景图像（多为车载视角）的空间理解评测集             | 真实世界空间理解 |
| ChartQA     | 图表问答                                                     | 图表问答         |
| InfoVQA     | 信息图表问答                                                 | 信息图表问答     |
